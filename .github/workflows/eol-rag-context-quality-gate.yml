name: EOL RAG Context - Quality Gate

on:
  push:
    branches: [ main, develop, feat/*, fix/* ]
    paths:
      - 'packages/eol-rag-context/**'
      - '.github/workflows/eol-rag-context-quality-gate.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'packages/eol-rag-context/**'

env:
  PACKAGE_PATH: packages/eol-rag-context
  PYTHON_VERSION: '3.11'
  COVERAGE_THRESHOLD: 80
  REDIS_HOST: localhost
  REDIS_PORT: 6379

jobs:
  # =========================================
  # Pre-flight Checks
  # =========================================
  pre-flight:
    name: ðŸ” Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      python-version: ${{ steps.setup.outputs.python-version }}
      package-changed: ${{ steps.changes.outputs.package }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 2
    
    - name: Detect changes
      uses: dorny/paths-filter@v2
      id: changes
      with:
        filters: |
          package:
            - 'packages/eol-rag-context/**'
            - '.github/workflows/**'
    
    - name: Setup outputs
      id: setup
      run: |
        echo "python-version=${{ env.PYTHON_VERSION }}" >> $GITHUB_OUTPUT
        
    - name: Validate project structure
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸ” Validating project structure..."
        
        # Check essential files exist
        required_files=(
          "pyproject.toml"
          "src/eol/rag_context/__init__.py"
          "tests/conftest.py"
          "tests/integration/conftest.py"
          "pytest.ini"
          "docker-compose.test.yml"
        )
        
        for file in "${required_files[@]}"; do
          if [ ! -f "$file" ]; then
            echo "âŒ Missing required file: $file"
            exit 1
          fi
        done
        
        echo "âœ… Project structure validation passed"

  # =========================================
  # Code Quality Checks
  # =========================================
  code-quality:
    name: ðŸ“Š Code Quality
    runs-on: ubuntu-latest
    needs: pre-flight
    if: needs.pre-flight.outputs.package-changed == 'true'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip packages
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-quality-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-quality-
          ${{ runner.os }}-pip-
    
    - name: Install quality tools
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt  # Includes black, ruff, mypy, etc.
        pip install bandit safety  # Additional security tools
    
    - name: Code formatting check (Black)
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸ” Checking code formatting with Black..."
        black --check --diff src/ tests/
    
    - name: Import sorting check (isort)
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸ” Checking import sorting with isort..."
        isort --check-only --diff src/ tests/
    
    - name: Linting (flake8)
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸ” Running flake8 linting..."
        flake8 src/ tests/ --max-line-length=100 --extend-ignore=E203,W503
    
    - name: Security scan (bandit)
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸ” Running security scan with bandit..."
        bandit -r src/ -f json -o security-report.json || true
        bandit -r src/ -ll
    
    - name: Dependency security check
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸ” Checking dependency security with safety..."
        # Extract dependencies from pyproject.toml for safety check
        pip freeze | safety check --stdin --json --output safety-report.json || true
        pip freeze | safety check --stdin
    
    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          ${{ env.PACKAGE_PATH }}/security-report.json
          ${{ env.PACKAGE_PATH }}/safety-report.json

  # =========================================
  # Unit Tests
  # =========================================
  unit-tests:
    name: ðŸ§ª Unit Tests
    runs-on: ubuntu-latest
    needs: pre-flight
    if: needs.pre-flight.outputs.package-changed == 'true'
    
    strategy:
      matrix:
        python-version: ['3.11', '3.12']
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip packages
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-${{ matrix.python-version }}-pip-unit-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-${{ matrix.python-version }}-pip-unit-
          ${{ runner.os }}-${{ matrix.python-version }}-pip-
    
    - name: Install dependencies
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        python -m pip install --upgrade pip
        pip install uv
        
        # Core testing dependencies
        uv pip install --system pytest pytest-asyncio pytest-cov pytest-timeout pytest-xdist
        
        # Install project dependencies from requirements files
        uv pip install --system -r requirements.txt
        uv pip install --system sentence-transformers  # For embeddings in tests
        
        # Note: unittest.mock is part of Python standard library (no install needed)
        
        echo "ðŸ“¦ Installed dependencies for Python ${{ matrix.python-version }}"
    
    - name: Run unit tests
      working-directory: ${{ env.PACKAGE_PATH }}
      env:
        PYTHONPATH: ${{ github.workspace }}/${{ env.PACKAGE_PATH }}/src
      run: |
        echo "ðŸ§ª Running unit tests with Python ${{ matrix.python-version }}..."
        
        python -m pytest \
          tests/test_config.py \
          tests/test_embeddings.py \
          tests/test_document_processor.py \
          tests/test_indexer.py \
          -v \
          --tb=short \
          --cov=eol.rag_context \
          --cov-report=term \
          --cov-report=xml:unit-coverage-${{ matrix.python-version }}.xml \
          --cov-report=html:unit-coverage-${{ matrix.python-version }} \
          --junit-xml=unit-test-results-${{ matrix.python-version }}.xml \
          -m "not integration"
    
    - name: Upload unit test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          ${{ env.PACKAGE_PATH }}/unit-test-results-${{ matrix.python-version }}.xml
          ${{ env.PACKAGE_PATH }}/unit-coverage-${{ matrix.python-version }}.xml
          ${{ env.PACKAGE_PATH }}/unit-coverage-${{ matrix.python-version }}/

  # =========================================
  # Integration Tests
  # =========================================
  integration-tests:
    name: ðŸ”„ Integration Tests
    runs-on: ubuntu-latest
    needs: [pre-flight, unit-tests]
    if: needs.pre-flight.outputs.package-changed == 'true'
    
    services:
      redis:
        image: redis/redis-stack:latest
        ports:
          - 6379:6379
          - 8001:8001
        options: >-
          --name redis-stack
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
          --health-start-period 10s
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip packages
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-integration-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-integration-
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        python -m pip install --upgrade pip
        pip install uv
        
        # Install all project dependencies for integration tests
        uv pip install --system pytest pytest-asyncio pytest-cov pytest-timeout
        uv pip install --system -r requirements.txt
        uv pip install --system sentence-transformers  # For embeddings
        uv pip install --system aioredis  # For async Redis operations
        
        # Optional dependencies for comprehensive testing
        uv pip install --system watchdog gitignore-parser || true
        
        echo "ðŸ“¦ Installed full dependencies for integration tests"
    
    - name: Wait for Redis
      run: |
        echo "â³ Waiting for Redis Stack to be fully ready..."
        timeout 30s bash -c 'until redis-cli -h localhost -p 6379 ping; do sleep 1; done'
        
        # Verify RediSearch module is loaded
        echo "ðŸ” Verifying RediSearch module..."
        redis-cli -h localhost -p 6379 MODULE LIST | grep search || {
          echo "âŒ RediSearch module not found in Redis Stack"
          redis-cli -h localhost -p 6379 MODULE LIST
          exit 1
        }
        
        echo "âœ… Redis Stack with RediSearch is ready"
    
    - name: Clear Redis data
      run: |
        echo "ðŸ§¹ Clearing Redis data for clean test state..."
        redis-cli -h localhost -p 6379 FLUSHALL
    
    - name: Run integration tests
      working-directory: ${{ env.PACKAGE_PATH }}
      env:
        PYTHONPATH: ${{ github.workspace }}/${{ env.PACKAGE_PATH }}/src
        REDIS_HOST: localhost
        REDIS_PORT: 6379
      run: |
        echo "ðŸ”„ Running integration tests..."
        
        python -m pytest tests/integration/ \
          -v \
          --tb=short \
          --cov=eol.rag_context \
          --cov-append \
          --cov-report=term \
          --cov-report=xml:integration-coverage.xml \
          --cov-report=html:integration-coverage \
          --junit-xml=integration-test-results.xml \
          --timeout=300 \
          -m integration
    
    - name: Integration test summary
      if: always()
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸ“Š Integration Test Summary:"
        
        if [ -f integration-test-results.xml ]; then
          python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('integration-test-results.xml')
          root = tree.getroot()
          tests = int(root.get('tests', 0))
          failures = int(root.get('failures', 0))
          errors = int(root.get('errors', 0))
          passed = tests - failures - errors
          
          print(f'ðŸ“Š Integration Tests: {passed}/{tests} passed ({passed/tests*100:.1f}%)')
          
          if failures > 0 or errors > 0:
              print(f'âŒ {failures} failures, {errors} errors')
          else:
              print('âœ… All integration tests passed!')
          "
        fi
    
    - name: Upload integration test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: integration-test-results
        path: |
          ${{ env.PACKAGE_PATH }}/integration-test-results.xml
          ${{ env.PACKAGE_PATH }}/integration-coverage.xml
          ${{ env.PACKAGE_PATH }}/integration-coverage/

  # =========================================
  # Performance & E2E Tests
  # =========================================
  performance-tests:
    name: âš¡ Performance Tests
    runs-on: ubuntu-latest
    needs: [pre-flight, integration-tests]
    if: needs.pre-flight.outputs.package-changed == 'true'
    
    services:
      redis:
        image: redis/redis-stack:latest
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        python -m pip install --upgrade pip
        pip install uv
        uv pip install --system pytest pytest-asyncio pytest-benchmark
        uv pip install --system -r requirements.txt
        uv pip install --system sentence-transformers  # For embeddings
    
    - name: Run performance tests
      working-directory: ${{ env.PACKAGE_PATH }}
      env:
        PYTHONPATH: ${{ github.workspace }}/${{ env.PACKAGE_PATH }}/src
        REDIS_HOST: localhost
        REDIS_PORT: 6379
      run: |
        echo "âš¡ Running performance benchmarks..."
        
        python -m pytest tests/integration/test_full_workflow_integration.py::TestFullWorkflowIntegration::test_performance_metrics \
          -v \
          --tb=short \
          --benchmark-json=performance-results.json \
          -m performance || true
        
        # Display performance results if available
        if [ -f performance-results.json ]; then
          echo "ðŸ“Š Performance Results:"
          python -c "
          import json
          with open('performance-results.json', 'r') as f:
              data = json.load(f)
              for benchmark in data.get('benchmarks', []):
                  name = benchmark['name']
                  stats = benchmark['stats']
                  mean = stats['mean']
                  print(f'âš¡ {name}: {mean:.3f}s average')
          " || echo "Performance data processing failed"
        fi
    
    - name: Upload performance results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: ${{ env.PACKAGE_PATH }}/performance-results.json

  # =========================================
  # Coverage Analysis & Quality Gate
  # =========================================
  coverage-gate:
    name: ðŸ“ˆ Coverage Quality Gate
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: always() && needs.pre-flight.outputs.package-changed == 'true'
    
    services:
      redis:
        image: redis/redis-stack:latest
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        python -m pip install --upgrade pip
        pip install uv
        uv pip install --system pytest pytest-asyncio pytest-cov
        uv pip install --system -r requirements.txt
        uv pip install --system sentence-transformers aioredis  # Additional for coverage tests
    
    - name: Wait for Redis
      run: |
        timeout 30s bash -c 'until redis-cli -h localhost -p 6379 ping; do sleep 1; done'
    
    - name: Generate comprehensive coverage
      working-directory: ${{ env.PACKAGE_PATH }}
      env:
        PYTHONPATH: ${{ github.workspace }}/${{ env.PACKAGE_PATH }}/src
        REDIS_HOST: localhost
        REDIS_PORT: 6379
      run: |
        echo "ðŸ“Š Generating comprehensive coverage report..."
        
        # Clear any existing coverage data
        rm -f .coverage*
        
        # Run all tests with coverage
        python -m pytest tests/ \
          --cov=eol.rag_context \
          --cov-branch \
          --cov-report=term-missing \
          --cov-report=xml:final-coverage.xml \
          --cov-report=html:final-coverage-html \
          --cov-report=json:final-coverage.json \
          --quiet
    
    - name: Coverage quality gate check
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸŽ¯ Checking coverage against quality gate..."
        
        python -c "
        import json
        import sys
        import xml.etree.ElementTree as ET
        
        # Read coverage from JSON (more reliable than XML parsing)
        try:
            with open('final-coverage.json', 'r') as f:
                data = json.load(f)
                coverage = data.get('totals', {}).get('percent_covered', 0)
        except:
            # Fallback to XML parsing
            tree = ET.parse('final-coverage.xml')
            root = tree.getroot()
            coverage = float(root.get('line-rate', 0)) * 100
        
        threshold = ${{ env.COVERAGE_THRESHOLD }}
        
        print(f'ðŸ“Š Final Coverage: {coverage:.1f}%')
        print(f'ðŸŽ¯ Required Threshold: {threshold}%')
        
        if coverage >= threshold:
            print(f'âœ… SUCCESS: Coverage {coverage:.1f}% meets threshold {threshold}%')
            print('::notice title=Coverage Success::Coverage target achieved!')
        else:
            gap = threshold - coverage
            print(f'âŒ FAILURE: Coverage {coverage:.1f}% is {gap:.1f}% below threshold {threshold}%')
            print(f'::error title=Coverage Failure::Coverage {coverage:.1f}% is below required {threshold}%')
            sys.exit(1)
        "
    
    - name: Generate coverage badge
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸ·ï¸ Generating coverage badge..."
        
        python -c "
        import json
        
        with open('final-coverage.json', 'r') as f:
            data = json.load(f)
            coverage = data.get('totals', {}).get('percent_covered', 0)
        
        # Determine badge color
        if coverage >= 80:
            color = 'brightgreen'
        elif coverage >= 60:
            color = 'yellow'
        else:
            color = 'red'
        
        # Create simple badge data
        badge_data = {
            'schemaVersion': 1,
            'label': 'coverage',
            'message': f'{coverage:.1f}%',
            'color': color
        }
        
        with open('coverage-badge.json', 'w') as f:
            json.dump(badge_data, f, indent=2)
        
        print(f'Generated coverage badge: {coverage:.1f}% ({color})')
        "
    
    - name: Upload coverage reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: final-coverage-reports
        path: |
          ${{ env.PACKAGE_PATH }}/final-coverage.xml
          ${{ env.PACKAGE_PATH }}/final-coverage.json
          ${{ env.PACKAGE_PATH }}/final-coverage-html/
          ${{ env.PACKAGE_PATH }}/coverage-badge.json
    
    - name: Comment coverage on PR
      if: github.event_name == 'pull_request'
      uses: py-cov-action/python-coverage-comment-action@v3
      with:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        MINIMUM_GREEN: ${{ env.COVERAGE_THRESHOLD }}
        MINIMUM_ORANGE: 60
        ANNOTATE_MISSING_LINES: true
        ANNOTATION_TYPE: warning

  # =========================================
  # Security & Compliance Gate
  # =========================================
  security-gate:
    name: ðŸ”’ Security Gate
    runs-on: ubuntu-latest
    needs: pre-flight
    if: needs.pre-flight.outputs.package-changed == 'true'
    
    permissions:
      actions: read
      contents: read
      security-events: write
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: ${{ env.PACKAGE_PATH }}
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security
      if: always()
      uses: github/codeql-action/upload-sarif@v3
      continue-on-error: true
      with:
        sarif_file: 'trivy-results.sarif'
        category: 'trivy-security-scan'
    
    - name: Display security scan summary
      if: always()
      run: |
        echo "ðŸ”’ Security scan completed"
        if [ -f trivy-results.sarif ]; then
          echo "âœ… SARIF report generated successfully"
          # Extract basic info from SARIF (if available)
          python -c "
          import json
          try:
              with open('trivy-results.sarif', 'r') as f:
                  data = json.load(f)
              runs = data.get('runs', [])
              if runs:
                  results = runs[0].get('results', [])
                  print(f'ðŸ“Š Security scan found {len(results)} issues')
              else:
                  print('ðŸ“Š No security issues found')
          except:
              print('ðŸ“Š Security scan completed (details in SARIF file)')
          " || echo "ðŸ“Š Security scan completed"
        else
          echo "âš ï¸ SARIF report not found"
        fi
    
    - name: Dependency vulnerability check
      working-directory: ${{ env.PACKAGE_PATH }}
      run: |
        echo "ðŸ” Checking for known vulnerabilities..."
        
        # Install safety and tomli for dependency checking
        pip install safety tomli
        
        # Create a temporary requirements file from pyproject.toml
        python -c "
        import tomli
        with open('pyproject.toml', 'rb') as f:
            data = tomli.load(f)
        deps = data.get('project', {}).get('dependencies', [])
        with open('temp-requirements.txt', 'w') as f:
            for dep in deps:
                f.write(dep + '\n')
        " 2>/dev/null || echo "Could not parse pyproject.toml"
        
        # Run safety check
        if [ -f temp-requirements.txt ]; then
            safety check -r temp-requirements.txt --json --output vulnerability-report.json || true
            safety check -r temp-requirements.txt || echo "âš ï¸ Vulnerabilities found - check report"
            rm temp-requirements.txt
        fi
    
    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: security-scan-results
        path: |
          trivy-results.sarif
          ${{ env.PACKAGE_PATH }}/vulnerability-report.json

  # =========================================
  # Final Quality Gate Decision
  # =========================================
  quality-gate:
    name: ðŸš¦ Quality Gate Decision
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, coverage-gate, security-gate]
    if: always() && needs.pre-flight.outputs.package-changed == 'true'
    
    steps:
    - name: Quality gate decision
      run: |
        echo "ðŸš¦ Making quality gate decision..."
        
        # Check each job result
        code_quality="${{ needs.code-quality.result }}"
        unit_tests="${{ needs.unit-tests.result }}"
        integration_tests="${{ needs.integration-tests.result }}"
        coverage_gate="${{ needs.coverage-gate.result }}"
        security_gate="${{ needs.security-gate.result }}"
        
        echo "ðŸ“Š Quality Gate Results:"
        echo "â€¢ Code Quality: $code_quality"
        echo "â€¢ Unit Tests: $unit_tests"
        echo "â€¢ Integration Tests: $integration_tests"
        echo "â€¢ Coverage Gate: $coverage_gate"
        echo "â€¢ Security Gate: $security_gate"
        
        # Determine overall result
        failed_gates=""
        
        if [ "$code_quality" = "failure" ]; then
          failed_gates="$failed_gates code-quality"
        fi
        
        if [ "$unit_tests" = "failure" ]; then
          failed_gates="$failed_gates unit-tests"
        fi
        
        if [ "$integration_tests" = "failure" ]; then
          failed_gates="$failed_gates integration-tests"
        fi
        
        if [ "$coverage_gate" = "failure" ]; then
          failed_gates="$failed_gates coverage-gate"
        fi
        
        if [ "$security_gate" = "failure" ]; then
          failed_gates="$failed_gates security-gate"
        fi
        
        if [ -n "$failed_gates" ]; then
          echo "âŒ QUALITY GATE FAILED"
          echo "Failed gates:$failed_gates"
          echo "::error title=Quality Gate Failed::The following quality gates failed:$failed_gates"
          exit 1
        else
          echo "âœ… QUALITY GATE PASSED"
          echo "::notice title=Quality Gate Success::All quality gates passed successfully!"
        fi
    
    - name: Quality gate summary
      if: always()
      run: |
        echo "## ðŸš¦ Quality Gate Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Gate | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|------|---------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.code-quality.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Coverage Gate | ${{ needs.coverage-gate.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Gate | ${{ needs.security-gate.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.code-quality.result }}" = "success" ] && 
           [ "${{ needs.unit-tests.result }}" = "success" ] && 
           [ "${{ needs.integration-tests.result }}" = "success" ] && 
           [ "${{ needs.coverage-gate.result }}" = "success" ] && 
           [ "${{ needs.security-gate.result }}" = "success" ]; then
          echo "ðŸŽ‰ **Overall Result: âœ… QUALITY GATE PASSED**" >> $GITHUB_STEP_SUMMARY
        else
          echo "ðŸ’¥ **Overall Result: âŒ QUALITY GATE FAILED**" >> $GITHUB_STEP_SUMMARY
        fi